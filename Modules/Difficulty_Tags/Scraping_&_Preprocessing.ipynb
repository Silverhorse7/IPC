{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v33c9k9H-Kxm",
        "outputId": "72a7f6f0-5b63-4d9e-8d20-b5c685d55b36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrHOt2Yo-xnM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "19UQVjED-7bN",
        "outputId": "fee2201b-25a1-4c35-c8a1-7beebfdbc78f"
      },
      "outputs": [],
      "source": [
        "# File paths for the CSV files\n",
        "file_paths = [\"/content/AMT10_train.csv\", \"/content/AMT10_test.csv\", \"/content/AMT10_validation.csv\"]\n",
        "\n",
        "# List to hold DataFrames for each CSV file\n",
        "dfs = []\n",
        "\n",
        "# Read each CSV file and append its DataFrame to the list\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Drop all but the first column\n",
        "combined_df = combined_df.iloc[:, :1]\n",
        "\n",
        "combined_df.head(10)\n",
        "\n",
        "# Extract numbers from each row\n",
        "contest_ids = []\n",
        "for index, row in combined_df.iterrows():\n",
        "    number = row.iloc[0].split('/')[0]\n",
        "    contest_ids.append(int(number))\n",
        "\n",
        "# Get unique numbers by converting the list to a set\n",
        "contest_ids = set(contest_ids)\n",
        "\n",
        "# Convert the set back to a sorted list\n",
        "contest_ids = sorted(contest_ids)\n",
        "\n",
        "print(len(contest_ids))\n",
        "print(contest_ids)\n",
        "\n",
        "missing_contest_ids = [num for num in range(1, 1957) if num not in contest_ids]\n",
        "\n",
        "print(len(missing_contest_ids))\n",
        "print(missing_contest_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzjjZ-SwFOvv"
      },
      "outputs": [],
      "source": [
        "# Function to extract the contest id from the problem link\n",
        "def extract_number(link):\n",
        "    match = re.search(r'/problem/(\\d+)/', link)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    else:\n",
        "        print(\"Contest ID not found in the link.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "ChXJ1StyFJyq",
        "outputId": "641976b4-f632-488c-e094-6dbef3d64306"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/content/codeforces_problems.csv\")\n",
        "\n",
        "# Apply the function to extract numbers from the problem_link column\n",
        "df['contest_id'] = df['problem_link'].apply(extract_number)\n",
        "\n",
        "# Filter the DataFrame based on whether the extracted numbers exist in the given list\n",
        "filtered_df = df[df['contest_id'].isin(missing_contest_ids)]\n",
        "\n",
        "print(len(filtered_df))\n",
        "filtered_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "EvU96d6PGTpQ",
        "outputId": "5e3ed6e7-8f0c-4edc-f39d-d5b9452a072d"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['id', 'time_limit', 'memory_limit', 'sample_input', 'sample_output', 'contest_id']\n",
        "filtered_df = filtered_df.drop(columns=columns_to_drop)\n",
        "\n",
        "filtered_df.rename(columns={'problem_text': 'description'}, inplace=True)\n",
        "filtered_df.rename(columns={'difficulty': 'rating'}, inplace=True)\n",
        "\n",
        "filtered_df['rating'] = filtered_df['rating'].astype(float)\n",
        "\n",
        "# Assuming df is your DataFrame and 'problem_link' is the column containing the links\n",
        "filtered_df['problem_link'] = filtered_df['problem_link'].str.replace('https://codeforces.com/problemset/problem/', '')\n",
        "filtered_df.rename(columns={'problem_link': ''}, inplace=True)\n",
        "\n",
        "filtered_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIvphl9nLqSp",
        "outputId": "7f2dded2-0bd4-4347-bc82-ecf622bbc82f"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWaah8rsL1Em"
      },
      "outputs": [],
      "source": [
        "# This is an interactive task.Rudolph is a scientist who studies alien life forms.\n",
        "# -> This is an interactive task. Rudolph is a scientist who studies alien life forms.\n",
        "def processing_dot_capitalize(sentences):\n",
        "    new_sentences = \"\"\n",
        "    prev = sentences[0]\n",
        "    for t in sentences:\n",
        "        if prev == '.' and t.isupper():\n",
        "            new_sentences += ' '\n",
        "        new_sentences += t\n",
        "        prev = t\n",
        "    return new_sentences\n",
        "\n",
        "# ( $$$ 1 \\\\le t_{i, j} \\\\le 10^6 $$$ )\n",
        "# -> ( $$$ 1 \\\\le t_{i, j} \\\\le 1000000 $$$ )\n",
        "def replace_exponent_notation(text):\n",
        "    # Function to replace exponent notation with numbers\n",
        "    def replace_exponent(match):\n",
        "        exponent = int(match.group(1))\n",
        "        return str(10 ** exponent)\n",
        "\n",
        "    # Replace \"10^4\" or \"10^5\" with the corresponding numbers\n",
        "    replaced_text = re.sub(r'10\\^(\\d+)', replace_exponent, text)\n",
        "\n",
        "    return replaced_text\n",
        "\n",
        "# there is a room in front of rudolph with $$$n$$$ different objects scattered around.\n",
        "# -> There is a room in front of Rudolph with $$$ n $$$ different objects scattered around.\n",
        "def add_spacing_between_dollar_signs(text):\n",
        "    # Regular expression pattern to add spacing between \"$$$\"\n",
        "    pattern = r'(?<=\\$\\$\\$)(?=\\S)|(?<=\\S)(?=\\$\\$\\$)'\n",
        "\n",
        "    # Add spacing between \"$$$\"\n",
        "    spaced_text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    return spaced_text\n",
        "\n",
        "# This is an interactive task.\n",
        "# -> this is an interactive task.\n",
        "def convert_to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def is_number(string):\n",
        "    return string.isdigit()\n",
        "\n",
        "# 2 \\\\cdot 100000\n",
        "# -> 200000\n",
        "def calculate_multiplication(text):\n",
        "    t = text.split()  # Split the input text into a list of words\n",
        "    new_text = []  # Initialize a new list for the modified text\n",
        "    count = 0  # Initialize a count to keep track of processed elements\n",
        "\n",
        "    # Iterate through the words in the input text\n",
        "    for i in range(len(t)):\n",
        "        if count > 0:\n",
        "            count -= 1\n",
        "            continue\n",
        "\n",
        "        # Check if the current word and the word after it form a multiplication expression\n",
        "        if not len(t) - i < 3 and is_number(t[i]) and is_number(t[i + 2]) and t[i + 1] == '\\cdot':\n",
        "            # Evaluate and append the result of the multiplication to the new text\n",
        "            new_text.append(str(eval(t[i] + '*' + t[i + 2])))\n",
        "            count = 2  # Skip the next two words as they have been processed in the multiplication\n",
        "        else:\n",
        "            new_text.append(t[i])  # Append the current word to the new text\n",
        "\n",
        "    return ' '.join(new_text)  # Join the modified words to form the final text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQWQz3OlL2dJ"
      },
      "outputs": [],
      "source": [
        "def preprocessing(text):\n",
        "    #text = remove_less_than_three_letters(text)\n",
        "    text = processing_dot_capitalize(text) # Run before \"lowercase\"\n",
        "    text = convert_to_lowercase(text)\n",
        "    text = add_spacing_between_dollar_signs(text)\n",
        "    text = replace_exponent_notation(text)\n",
        "    text = calculate_multiplication(text) # optional\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRdL2JhQL3rx"
      },
      "outputs": [],
      "source": [
        "def split_sentences(sentences):\n",
        "    return sent_tokenize(sentences)\n",
        "\n",
        "def split_words(sentence):\n",
        "    return word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owHw6n8fL4th"
      },
      "outputs": [],
      "source": [
        "def lemmatization(tokens):\n",
        "    # Initialize the WordNet Lemmatizer\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize each word in the list of tokens as verbs ('v' indicates verb lemmatization)\n",
        "    tokens = [lmtzr.lemmatize(word, 'v') for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_words = []  # Initialize a list to store filtered words\n",
        "    stopwords = nltk.corpus.stopwords.words('english')  # Get the list of English stopwords\n",
        "    stopwords = [item for item in stopwords if len(item) > 1]  # Filter out single-letter stopwords\n",
        "\n",
        "    for word in tokens:\n",
        "        # If the individual word is not in the stopwords list, add it to the filtered_words list\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    return filtered_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udzbIGHlL6AN"
      },
      "outputs": [],
      "source": [
        "def get_preprocessing_sentence(tokens):\n",
        "    filtered_words = remove_stopwords(tokens)\n",
        "    filtered_words = lemmatization(filtered_words)\n",
        "    return ' '.join(filtered_words).replace('$ $ $', '$$$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSrNBE1UL7AK"
      },
      "outputs": [],
      "source": [
        "def get_preprocessed_sentence(sentences):\n",
        "    new_sentences = []\n",
        "\n",
        "    sentences = preprocessing(sentences)\n",
        "    sentences_list = split_sentences(sentences)\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        tokens = split_words(sentence)\n",
        "        preprocessed_sentence = get_preprocessing_sentence(tokens)\n",
        "        if preprocessed_sentence[-1] == '.':\n",
        "            preprocessed_sentence = preprocessed_sentence[:-2]\n",
        "        new_sentences.append(preprocessed_sentence.replace(' , ', ' '))\n",
        "    return new_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ-9edkoL9NK"
      },
      "outputs": [],
      "source": [
        "filtered_df = filtered_df.dropna(subset=['description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kU_2AbvMCFs",
        "outputId": "d9c36193-1171-470e-8189-66ff6193696c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "new_description = []\n",
        "for description in tqdm(filtered_df['description'].values):\n",
        "    new_description.append(get_preprocessed_sentence(description))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-GuQc6IMIIZ"
      },
      "outputs": [],
      "source": [
        "filtered_df['description'] = new_description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rrj1tYiNGMy"
      },
      "outputs": [],
      "source": [
        "# Function to convert a list to a string\n",
        "def list_to_string(lst):\n",
        "    return ' '.join(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xqYhmlHaMJUa",
        "outputId": "353dbf81-9539-4cb5-98c5-49e6f1b479cb"
      },
      "outputs": [],
      "source": [
        "filtered_df['description'] = filtered_df['description'].apply(list_to_string)\n",
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c69D0ya3MUnF"
      },
      "outputs": [],
      "source": [
        "def dollar_processing(arr):\n",
        "    # Initialize a new array\n",
        "    new_arr = []\n",
        "\n",
        "    # Replace consecutive '$' with '$$$'\n",
        "    i = 0\n",
        "    while i < len(arr):\n",
        "        if i + 2 < len(arr) and arr[i] == '$' and arr[i + 1] == '$' and arr[i + 2] == '$':\n",
        "            new_arr.append('$$$')\n",
        "            i += 3  # Process three '$' and increase the index by 3\n",
        "        else:\n",
        "            new_arr.append(arr[i])\n",
        "            i += 1\n",
        "    return new_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwKRPX_oMVL6"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "def tokenizing_sentences(sentences):\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = get_tokenizer(\"spacy\")\n",
        "\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence)\n",
        "        new_sentences.append(dollar_processing(tokens))\n",
        "\n",
        "    return new_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db1_piD-MZD0",
        "outputId": "ae385c13-5d02-4680-bb49-b7682e7321c6"
      },
      "outputs": [],
      "source": [
        "filtered_df['description'] = tokenizing_sentences(filtered_df['description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "00XVZYNrMbdf",
        "outputId": "49d25ffd-7479-4cdf-c6fd-eadf2538eb9b"
      },
      "outputs": [],
      "source": [
        "filtered_df['description'] = filtered_df['description'].apply(list_to_string)\n",
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1UD2AX3Mfej"
      },
      "outputs": [],
      "source": [
        "filtered_df.to_csv('finetuning_data.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
